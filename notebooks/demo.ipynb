{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GraphPromptor Usage Example\n",
    "\n",
    "This notebook demonstrates the end-to-end workflow of the GraphPromptor framework, from loading a Knowledge Graph to generating an answer using an LLM guided by the graph information.\n",
    "\n",
    "The workflow follows the \"Retrieve → Embed → Reason\" methodology, utilizing the `RetrievalModule`, `KnowledgeAdapter`, and `ReasoningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "from lightprof.utils import load_kg_from_triples, get_gemini_tokenizer\n",
    "from lightprof.retrieval import RetrievalModule\n",
    "from lightprof.adapter import KnowledgeAdapter, Subgraph # Import Subgraph type alias\n",
    "from lightprof.reasoning import ReasoningModule\n",
    "\n",
    "import networkx as nx\n",
    "from typing import List, Tuple, Dict, Any\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load Knowledge Graph\n",
    "\n",
    "Load the Knowledge Graph from a TSV file containing triples (head, relation, tail). The `load_kg_from_triples` function from `lightprof.utils` is used for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the knowledge graph triples file\n",
    "kg_filepath: str = 'data/freebase_triples.tsv'\n",
    "\n",
    "# Load the Knowledge Graph\n",
    "try:\n",
    "    kg: nx.DiGraph = load_kg_from_triples(filepath=kg_filepath)\n",
    "    print(f\"Successfully loaded Knowledge Graph from {kg_filepath}\")\n",
    "    print(f\"Number of nodes: {kg.number_of_nodes()}\")\n",
    "    print(f\"Number of edges: {kg.number_of_edges()}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: KG file not found at {kg_filepath}. Please ensure the file exists.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred while loading the KG: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initialize Modules\n",
    "\n",
    "Initialize the three core modules: `RetrievalModule`, `KnowledgeAdapter`, and `ReasoningModule`.\n",
    "\n",
    "- The `RetrievalModule` requires the loaded Knowledge Graph and placeholder components for `hop_predictor` and `entity_linker` if actual trained models are not available.\n",
    "- The `KnowledgeAdapter` is initialized with parameters for the text encoder (BERT) and embedding dimensions.\n",
    "- The `ReasoningModule` is initialized with the LLM model name and a hard prompt template. Note that the ReasoningModule requires the `GOOGLE_API_KEY` environment variable to be set for the Gemini model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize tokenizer (used by RetrievalModule and potentially others)\n",
    "tokenizer: Any = get_gemini_tokenizer()\n",
    "\n",
    "# Initialize placeholder or dummy components for RetrievalModule\n",
    "# In a real scenario, these would be trained models\n",
    "hop_predictor: Any = None  # Replace with your trained hop predictor\n",
    "entity_linker: Any = None  # Replace with your trained entity linker\n",
    "path_ranker: Any = None    # Replace with your trained path ranker\n",
    "\n",
    "# Initialize RetrievalModule\n",
    "retriever: RetrievalModule = RetrievalModule(\n",
    "    kg=kg,\n",
    "    hop_predictor=hop_predictor,\n",
    "    entity_linker=entity_linker,\n",
    "    path_ranker=path_ranker\n",
    ")\n",
    "\n",
    "# Initialize KnowledgeAdapter\n",
    "# Parameters like struct_emb_dim and llm_embed_dim should match your training setup\n",
    "adapter: KnowledgeAdapter = KnowledgeAdapter(\n",
    "    bert_model_name='bert-base-uncased', # Text encoder model\n",
    "    struct_emb_dim=128,                 # Structural embedding dimension\n",
    "    llm_embed_dim=768                   # Target LLM embedding dimension\n",
    ")\n",
    "\n",
    "# Initialize ReasoningModule\n",
    "# Ensure GOOGLE_API_KEY environment variable is set for Gemini models\n",
    "reasoner: ReasoningModule = ReasoningModule(\n",
    "    llm_model_name='gemini-2.5-flash-preview-04-17', # LLM model name\n",
    "    hard_template=\"Answer the question: {question}\\nKnowledge Graph Info: {kg_info}\\nAnswer:\" # Hard prompt template\n",
    ")\n",
    "\n",
    "print(\"Modules initialized.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Define Sample Question\n",
    "\n",
    "Define a sample question that the GraphPromptor will attempt to answer using the Knowledge Graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the question\n",
    "question: str = \"Which drugs did Lindsay Lohan abuse?\"\n",
    "\n",
    "print(f\"Sample Question: {question}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Run Inference Process\n",
    "\n",
    "Execute the core GraphPromptor workflow:\n",
    "\n",
    "1.  **Retrieve Paths:** Use the `RetrievalModule` to find relevant paths in the KG based on the question.\n",
    "2.  **Encode Paths:** Use the `KnowledgeAdapter` to convert the retrieved paths into soft prompt embeddings.\n",
    "3.  **Generate Answer:** Use the `ReasoningModule` to generate the final answer by combining the question and the soft prompt embeddings (passed as the subgraph structure in this implementation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Step 4.1: Retrieve Paths ---\n",
    "# Use the retriever to find relevant paths in the KG\n",
    "print(\"Retrieving paths...\")\n",
    "paths: Subgraph = retriever.retrieve_paths(question=question)\n",
    "\n",
    "print(f\"Retrieved {len(paths)} paths.\")\n",
    "if paths:\n",
    "    print(\"Example retrieved path:\")\n",
    "    for triple in paths[0]:\n",
    "        print(f\"  ({triple[0]}, {triple[1]}, {triple[2]})\")\n",
    "\n",
    "# --- Step 4.2: Encode Paths into Soft Prompts ---\n",
    "# Use the adapter to convert paths into fused embeddings\n",
    "# Note: The current ReasoningModule implementation uses the subgraph structure string,\n",
    "# not the actual soft prompt embeddings from the adapter. The adapter call is included\n",
    "# here to show the intended workflow step, but its output (`soft`) is not directly\n",
    "# used by the reasoner's `answer` method in its current form.\n",
    "print(\"Encoding paths into soft prompts...\")\n",
    "soft: torch.Tensor = adapter(subgraph=paths)\n",
    "\n",
    "print(f\"Generated soft prompt embeddings with shape: {soft.shape}\")\n",
    "\n",
    "# --- Step 4.3: Generate Answer ---\n",
    "# Use the reasoner to generate the answer\n",
    "# The reasoner's `answer` method expects the subgraph structure, not the soft embeddings.\n",
    "# This is a simplification in the current ReasoningModule implementation.\n",
    "print(\"Generating answer...\")\n",
    "answer: str = reasoner.answer(question=question, subgraph=paths)\n",
    "\n",
    "print(\"Inference process complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Print Final Answer\n",
    "\n",
    "Display the answer generated by the `ReasoningModule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the final answer\n",
    "print(\"\\nFinal Answer:\")\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
